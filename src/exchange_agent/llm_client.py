"""
Provider-agnostic LLM client.
Supports Anthropic and OpenAI â€” auto-detects based on available API keys.
Override with LLM_PROVIDER env var: "anthropic" or "openai"
"""
import os
import asyncio
from typing import Optional


class LLMClient:
    def __init__(self):
        self.provider = self._detect_provider()
        self.client = self._init_client()

    def _detect_provider(self) -> str:
        # Allow explicit override
        forced = os.getenv("LLM_PROVIDER", "").lower()
        if forced in ("anthropic", "openai"):
            return forced

        # Auto-detect based on available keys
        if os.getenv("ANTHROPIC_API_KEY"):
            return "anthropic"
        if os.getenv("OPENAI_API_KEY"):
            return "openai"

        raise RuntimeError("No LLM API key found. Set ANTHROPIC_API_KEY or OPENAI_API_KEY.")

    def _init_client(self):
        if self.provider == "anthropic":
            import anthropic
            return anthropic.Anthropic(api_key=os.getenv("ANTHROPIC_API_KEY"))
        else:
            from openai import OpenAI
            return OpenAI(api_key=os.getenv("OPENAI_API_KEY"))

    async def complete(self, system: str, user: str, max_tokens: int = 500, temperature: float = 0.7) -> str:
        """Single unified interface for both providers."""
        if self.provider == "anthropic":
            response = await asyncio.to_thread(
                self.client.messages.create,
                model=os.getenv("ANTHROPIC_MODEL", "claude-sonnet-4-20250514"),
                max_tokens=max_tokens,
                system=system,
                messages=[{"role": "user", "content": user}]
            )
            return response.content[0].text.strip()
        else:
            response = await asyncio.to_thread(
                self.client.chat.completions.create,
                model=os.getenv("OPENAI_MODEL", "gpt-4o-mini"),
                messages=[
                    {"role": "system", "content": system},
                    {"role": "user", "content": user}
                ],
                temperature=temperature,
                max_tokens=max_tokens
            )
            return response.choices[0].message.content.strip()


# Singleton
_llm_client: Optional[LLMClient] = None


def get_llm_client() -> LLMClient:
    global _llm_client
    if _llm_client is None:
        _llm_client = LLMClient()
    return _llm_client
